{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A Basic 2 Layer Neural Net Implementation (Using Numpy)\n",
    "\n",
    "### This code is a follow along/documentation of the code by Siraj Raval on https://www.youtube.com/watch?v=h3l4qz76JhQ&t=28s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Importing dependencies :\n",
    "\n",
    "- The only dependency we will need for running this network is numpy (for mathematically manipulating multidimensional arrays and matrices.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Defining the non-linearity :\n",
    "\n",
    "- For this example, the sigmoid non-linearity function is utilized. It maps any value that is passed in the function to a value between 0 & 1. This is very helpful as it helps convert numbers to probabilities.\n",
    "- The sigmoid function can generate derivative of a sigmoid when the derivative is set to \"True\".\n",
    "\n",
    "##### A very helpful property of sigmoid function is that its output can be used to create its derivative. For example, if the sigmoid has an output of \"x\", then the derivative is calculated simply as x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# non linearity that is utilized to gauge output confidence.\n",
    "# values are between 0 & 1. The sigmoid function can also generate\n",
    "# a derivative if (deriv=True)\n",
    "def sigmoidFn(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Uncomment the code below to see slope calculation in action as we provide an input(x1) array.\n",
    "# x1 = np.array([[2],[0],[-1]])\n",
    "\n",
    "# When we pass x1 values through the sigmoidFunction, we get corresponding points(y1) on the sigmoid graph.\n",
    "# y1 = sigmoidFn(x1, False)\n",
    "\n",
    "# The values that are returned above(y1) can now be used to calculate the derivative(slopes) of those points\n",
    "# slopes = sigmoidFn(y1, True)\n",
    "\n",
    "# print(\"x1 values: \", x1)\n",
    "# print(\"y1 Values: \",y1)\n",
    "# print(\"slope values: \",slopes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Initializing the input matrix using numpy :\n",
    "\n",
    "- For all intents and purposes, I will be creating a very basic input-dataset. It is a simple 4 X 3 matrix (4 rows by 3 columns)\n",
    "\n",
    "\n",
    "- Each row is a \"training-example\" and each column is an input \"node\" that is fed into the network. This means, the network has 3 inputs and 4 training examples per input which the network can be trained from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0,1], \n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "\n",
    "print(\"X\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Initializing the output matrix using numpy :\n",
    "\n",
    "- In accordance with the simple training data, a simple output matrix is defined (so that the network can be quickly trained), which is a 1 X 4 (1 row by 4 columns) array.\n",
    "\n",
    "\n",
    "- The next step would be transpose the array, which would change the shape of the matrix above to a 4 X 1. This is done to ensure the output is in accordance with the input. i.e, Each row is a training example and each column is an output node.\n",
    "### The above statement means that the output layer has 4 inputs and one output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.array([[0,0,1,1]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Random seeding \n",
    "\n",
    "- Seeding is a numpy method so that there is a uniform random distribution of numbers everytime the network is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Defining a weight matrix for this Neural Network :\n",
    "\n",
    "- Because there are only 2 layers in the neural network, the input layer & the output layer, only one weight matrix is necessary to connect the two layers.\n",
    "\n",
    "- The dimension of the weight matrix is 3 X 1 because the input layer(as discussed in step 3) is of size 3 and the output layer(as discussed in step 4) is a size of 1.\n",
    "\n",
    "- The initialization of the weight is very theory intensive, but as best practice for this simple example, we initialize the weight randomly with a zero mean.\n",
    "    ##### \"zero mean\" means that the sum of the entries vector divided by the dimension of the matrix equates to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the weight matrix randomly.\n",
    "weightMatrix = 2*np.random.random((3,1)) - 1\n",
    "# print weightMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7 - Training the Neural Network :\n",
    "\n",
    "- We create a \"for\" loop that iterates many times over our training data defined in step 3 & 4 to optimize the network to the specified dataset.\n",
    "\n",
    "### Forward Propagation :\n",
    "\n",
    "- We are going to utilize a technique known as full-batch training which will process all the input data(defined as X in step 3) at the same time.\n",
    "\n",
    "- We then proceed towards letting the netword predict (I use the term predict loosely because each prediction is a constant iteration towards an even better prediction) the output based on the input.\n",
    "\n",
    "- The next bit of code below can be interpreted in 2 steps. Firstly, our inputLayer matrix, which is X(defined in step 3) of size 4 X 3 undergoes dot product (matrix multiplication) with our initialized weightMatrix(from step 6) of size 3 X 1. As a rule, the result of the dot product is a matrix of size 4 X 1.\n",
    "```python \n",
    "outputLayer = sigmoidFn(np.dot(inputLayer,weightMatrix))\n",
    "```\n",
    "\n",
    "- Another important thing occurance to note in the above line of code is that the output of the dot product passes through the sigmoidFn declared way up in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "1st Iteration Output :\n",
      "-----------------------\n",
      "[[ 0.00287634]\n",
      " [ 0.00234601]\n",
      " [ 0.99808547]\n",
      " [ 0.99765245]]\n",
      "-----------------------\n",
      "1st iteration Output Error :\n",
      "-----------------------\n",
      "[[-0.00287634]\n",
      " [-0.00234601]\n",
      " [ 0.00191453]\n",
      " [ 0.00234755]]\n",
      "-----------------------\n",
      "1000th Iteration Output :\n",
      "-----------------------\n",
      "[[ 0.0028633 ]\n",
      " [ 0.00233538]\n",
      " [ 0.99809414]\n",
      " [ 0.9976631 ]]\n",
      "-----------------------\n",
      "1000th iteration Output Error :\n",
      "-----------------------\n",
      "[[-0.0028633 ]\n",
      " [-0.00233538]\n",
      " [ 0.00190586]\n",
      " [ 0.0023369 ]]\n",
      "-----------------------\n",
      "5000th Iteration Output :\n",
      "-----------------------\n",
      "[[ 0.00281278]\n",
      " [ 0.00229422]\n",
      " [ 0.99812771]\n",
      " [ 0.99770431]]\n",
      "-----------------------\n",
      "5000th iteration Output Error :\n",
      "-----------------------\n",
      "[[-0.00281278]\n",
      " [-0.00229422]\n",
      " [ 0.00187229]\n",
      " [ 0.00229569]]\n",
      "-----------------------\n",
      "Final Trained Output :\n",
      "-----------------------\n",
      "[[ 0.00275326]\n",
      " [ 0.00224572]\n",
      " [ 0.99816728]\n",
      " [ 0.99775287]]\n",
      "-----------------------\n",
      "Final Training Error :\n",
      "-----------------------\n",
      "[[-0.00275326]\n",
      " [-0.00224572]\n",
      " [ 0.00183272]\n",
      " [ 0.00224713]]\n"
     ]
    }
   ],
   "source": [
    "for iter in range(10000):\n",
    "    # forward propagation\n",
    "    inputLayer = X\n",
    "    outputLayer = sigmoidFn(np.dot(inputLayer,weightMatrix))\n",
    "#     Print out the following at every 500th iteration.\n",
    "#     if iter % 500 == 0:\n",
    "#         print(\"inputputLayer:\",inputLayer)\n",
    "#         print(\"Weight-Matrix:\",weightMatrix)\n",
    "#         print(\"DotProduct:\",np.dot(inputLayer, weightMatrix))\n",
    "#         print(\"outputLayer\", outputLayer)\n",
    "    \n",
    "                #################\n",
    "                #               #\n",
    "                # Read STEP : 8 #\n",
    "                #     Below     #\n",
    "                #               #\n",
    "                #################\n",
    "    outputLayerError = Y - outputLayer\n",
    "#     Print out the OutputLayerError at every 500th iteration.\n",
    "#     if iter % 500 == 0:     \n",
    "#         print(\"OutputLayerError: \",outputLayerError)\n",
    "\n",
    "                #################\n",
    "                #               #\n",
    "                # Read STEP : 9 #\n",
    "                #     Below     #\n",
    "                #               #\n",
    "                #################\n",
    "    errorBasedChange = outputLayerError * sigmoidFn(outputLayer, True)\n",
    "#     Print out the OutputLayer's Derivative at every 500th iteration.\n",
    "#     if iter % 500 == 0:\n",
    "#         print(\"Derivative of the outputLayer:\", sigmoidFn(outputLayer, True))\n",
    "#         print(\"Change based on outputLayerError:\",errorBasedChange)\n",
    "        \n",
    "        \n",
    "                #################\n",
    "                #               #\n",
    "                # Read STEP : 10#\n",
    "                #     Below     #\n",
    "                #               #\n",
    "                #################\n",
    "    weightMatrix += np.dot(inputLayer.T, errorBasedChange)\n",
    "#     if iter % 500 == 0:\n",
    "#     print(\"T\",inputLayer.T)\n",
    "#     print(\"E\",errorBasedChange)\n",
    "#     print(\"W\",weightMatrix)\n",
    "#     print(\"Dot Product:\", np.dot(inputLayer.T, errorBasedChange))\n",
    "#     print(\"Updated Weight Matrix\", weightMatrix)\n",
    "\n",
    "    # The calculated output error for the first iteration\n",
    "    if iter == 1:\n",
    "        print(\"-----------------------\")\n",
    "        print(\"1st Iteration Output :\")\n",
    "        print(\"-----------------------\")\n",
    "        print(outputLayer)\n",
    "\n",
    "        print(\"-----------------------\")\n",
    "        print(\"1st iteration Output Error :\")\n",
    "        print(\"-----------------------\")\n",
    "        print(outputLayerError)\n",
    "\n",
    "    if iter == 1000:\n",
    "        print(\"-----------------------\")\n",
    "        print(\"1000th Iteration Output :\")\n",
    "        print(\"-----------------------\")\n",
    "        print(outputLayer)\n",
    "\n",
    "        print(\"-----------------------\")\n",
    "        print(\"1000th iteration Output Error :\")\n",
    "        print(\"-----------------------\")\n",
    "        print(outputLayerError)\n",
    "\n",
    "    if iter == 5000:\n",
    "        print(\"-----------------------\")\n",
    "        print(\"5000th Iteration Output :\")\n",
    "        print(\"-----------------------\")\n",
    "        print(outputLayer)\n",
    "\n",
    "        print(\"-----------------------\")\n",
    "        print(\"5000th iteration Output Error :\")\n",
    "        print(\"-----------------------\")\n",
    "        print(outputLayerError)\n",
    "\n",
    "print(\"-----------------------\")\n",
    "print(\"Final Trained Output :\")\n",
    "print(\"-----------------------\")\n",
    "print(outputLayer)\n",
    "\n",
    "print(\"-----------------------\")\n",
    "print(\"Final Training Error :\")\n",
    "print(\"-----------------------\")\n",
    "print(outputLayerError)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Evaluating calculation error :\n",
    "\n",
    "\n",
    "- The code under step 8 simply shows a means to calculate the accuracy of our predicted value(outputLayer) compared to the actual, predefined output value(Y).\n",
    "- Remember, that Y is a 4 X 1 matrix(As defined in step 4). The outputLayer is also a 4 X 1 matrix, therefore it is fairly intuitive that in order to calculate the error, we would have to subtract the two matrices.\n",
    "\n",
    "#### Uncomment parts of the code in step 8, to check out the various values of the  and make sure the calculations make sense to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 9 - Calculating the \"Error Based Change\" :\n",
    "\n",
    "- To understand this step fully, we can take it step-by-step.\n",
    "    - Firstly, after calculating the derivative sigmoidFn(outputLayer, True), each calculated value is always between 0 & 1. That is a property of the sigmoid function.\n",
    "    - Secondly, we multiply the returned 4 X 1 matrix consisting of the derivative to the outputLayerError matrix, which is also a 4 X 1 matrix.\n",
    "\n",
    "###### What the second step(multiplying the derivative matrix with the outputLayerError) does is that it multiplies the derivatves to the error \"elementwise\" and that helps to reduce the errors of predictions that carry a high confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Updating the weight matrix based on previously achieved results :\n",
    "\n",
    "- This update step is highly dependent on how much of a difference there is between the pre-defined output(Y) and the estimated output(outputLayer).\n",
    "- If one thinks about it intuitively, it makes sense that if a weight is already accurately predicting the correct output value, it does not need to be tampered with much.\n",
    "- In the final line, you can see, that after 10000 iterations, the outputLayerError has the lowest error compared to the 1st, 1000th and 5000th iteration errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2 - Adding a 3rd Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Random Weight Matrix  Syn0\n",
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "Initial Random Weight Matrix Syn1\n",
      "Output for the j iteration\n",
      "[[ 0.47070349]\n",
      " [ 0.48716227]\n",
      " [ 0.54111272]\n",
      " [ 0.54275466]]\n",
      "Error:\n",
      "0.496295788944\n",
      "Output for the j iteration\n",
      "[[ 0.46828579]\n",
      " [ 0.48584984]\n",
      " [ 0.53898089]\n",
      " [ 0.54126706]]\n",
      "Error:\n",
      "0.496180526782\n",
      "Output for the j iteration\n",
      "[[ 0.20672607]\n",
      " [ 0.63950689]\n",
      " [ 0.71254865]\n",
      " [ 0.58436571]]\n",
      "Error:\n",
      "0.359759062286\n",
      "Output for the j iteration\n",
      "[[ 0.01980049]\n",
      " [ 0.97453961]\n",
      " [ 0.97847519]\n",
      " [ 0.0297021 ]]\n",
      "Error:\n",
      "0.0241219498915\n",
      "Output for the j iteration\n",
      "[[ 0.00473102]\n",
      " [ 0.99397271]\n",
      " [ 0.9946546 ]\n",
      " [ 0.00705414]]\n",
      "Error:\n",
      "0.00578945986251\n",
      "Final Output at the 60,000th iteration\n",
      "[[ 0.00260572]\n",
      " [ 0.99672209]\n",
      " [ 0.99701711]\n",
      " [ 0.00386759]]\n",
      "Final Error\n",
      "0.00318353073559\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "            [1],\n",
    "            [1],\n",
    "            [0]])\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "print(\"Initial Random Weight Matrix  Syn0\")\n",
    "print(syn0)\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n",
    "print(\"Initial Random Weight Matrix Syn1\")\n",
    "# print(syn1)\n",
    "\n",
    "for j in range(60000):\n",
    "\n",
    "    # Feed forward through layers 0, 1, and 2\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    "\n",
    "    # how much did we miss the target value?\n",
    "    l2_error = y - l2\n",
    "\n",
    "    if (j==1 or j==2 or j==200 or j==2000 or j==20000):\n",
    "#         print(\"Weight Matrix for Syn0 in second iteration\")\n",
    "#         print(syn0)\n",
    "#         print(\"Weight Matrix for Syn1 in second iteration\")\n",
    "#         print(syn1)\n",
    "        print(\"Output for the j iteration\")\n",
    "        print(l2)\n",
    "#         print(\"l2_Error in the second iteration\")\n",
    "#         print(l2_error)\n",
    "        print(\"Error:\")\n",
    "        print(str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
    "\n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "\n",
    "print(\"Final Output at the 60,000th iteration\")\n",
    "print(l2)\n",
    "print(\"Final Error\")\n",
    "print(str(np.mean(np.abs(l2_error))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
